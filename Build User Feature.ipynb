{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys, os, gc, types\n",
    "import time\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_paths = [\n",
    "    \"/data/kaggle-instacart/\",\n",
    "    \"/Users/jiayou/Dropbox/珺珺的程序/Kaggle/Instacart/\",\n",
    "    \"/Users/jiayou/Dropbox/Documents/珺珺的程序/Kaggle/Instacart/\"\n",
    "]\n",
    "root = None\n",
    "for p in root_paths:\n",
    "    if os.path.exists(p):\n",
    "        root = p\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tick_tock:\n",
    "    def __init__(self, process_name, verbose=1):\n",
    "        self.process_name = process_name\n",
    "        self.verbose = verbose\n",
    "    def __enter__(self):\n",
    "        if self.verbose:\n",
    "            print(self.process_name + \" starts...\")\n",
    "            self.begin_time = time.time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.verbose:\n",
    "            end_time = time.time()\n",
    "            print('{} done: {:.2f}s'.format(self.process_name, end_time - self.begin_time))\n",
    "            \n",
    "def ka_add_groupby_features_1_vs_n(df, group_columns_list, agg_dict, only_new_feature=True):\n",
    "    try:\n",
    "        if type(group_columns_list) == list:\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(k + \"should be a list\")\n",
    "    except TypeError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    df_new = df.copy()\n",
    "    grouped = df_new.groupby(group_columns_list)\n",
    "\n",
    "    the_stats = grouped.agg(agg_dict)\n",
    "    the_stats.columns = the_stats.columns.droplevel(0)\n",
    "    the_stats.reset_index(inplace=True)\n",
    "    if only_new_feature:\n",
    "        df_new = the_stats\n",
    "    else:\n",
    "        df_new = pd.merge(left=df_new, right=the_stats, on=group_columns_list, how='left')\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug_name(s, ms):\n",
    "    return 'aug{}-{}'.format(s, ms)\n",
    "\n",
    "def load_data(root, aug = None, down_sample = None):\n",
    "    if aug is None:\n",
    "        pf = os.path.join(root, 'order_products__prior.csv')\n",
    "        tf = os.path.join(root, 'order_products__train.csv')\n",
    "        of = os.path.join(root, 'orders.csv')\n",
    "    else:\n",
    "        pf = os.path.join(root, 'aug', 'order_products__prior.{}.csv'.format(aug))\n",
    "        tf = os.path.join(root, 'aug', 'order_products__train.{}.csv'.format(aug))\n",
    "        of = os.path.join(root, 'aug', 'orders.{}.csv'.format(aug))\n",
    "    priors = pd.read_csv(pf, \n",
    "                     dtype={\n",
    "                            'order_id': np.int32,\n",
    "                            'product_id': np.uint16,\n",
    "                            'add_to_cart_order': np.int16,\n",
    "                            'reordered': np.int8})\n",
    "    train = pd.read_csv(tf, \n",
    "                    dtype={\n",
    "                            'order_id': np.int32,\n",
    "                            'product_id': np.uint16,\n",
    "                            'add_to_cart_order': np.int16,\n",
    "                            'reordered': np.int8})\n",
    "    orders = pd.read_csv(of, \n",
    "                         dtype={\n",
    "                                'order_id': np.int32,\n",
    "                                'user_id': np.int64,\n",
    "                                'eval_set': 'category',\n",
    "                                'order_number': np.int16,\n",
    "                                'order_dow': np.int8,\n",
    "                                'order_hour_of_day': np.int8,\n",
    "                                'days_since_prior_order': np.float32})\n",
    "    \n",
    "    if down_sample is not None:\n",
    "        priors = priors.merge(orders[['order_id', 'user_id']], on='order_id', how='left')\n",
    "        train = train.merge(orders[['order_id', 'user_id']], on='order_id', how='left')\n",
    "\n",
    "        orders = orders[orders.user_id % down_sample == 0]\n",
    "        priors = priors[priors.user_id % down_sample == 0]\n",
    "        train = train[train.user_id % down_sample == 0]\n",
    "\n",
    "        priors.drop('user_id', inplace = True, axis=1)\n",
    "        train.drop('user_id', inplace = True, axis=1)\n",
    "    \n",
    "    return priors, train, orders\n",
    "    \n",
    "    \n",
    "def load_features(root):\n",
    "    products = pd.read_csv(root + 'products.csv')\n",
    "    prod_feature = pd.read_csv(os.path.join(root, 'feature_prod.csv'))\n",
    "    prod_dow_feature = pd.read_csv(os.path.join(root, 'feature_prod_dow.csv'))\n",
    "    prod_hod_feature = pd.read_csv(os.path.join(root, 'feature_prod_hod.csv'))\n",
    "    category_feature = pd.read_csv(os.path.join(root, 'feature_category.csv'))\n",
    "    \n",
    "    return products, prod_feature, prod_dow_feature, prod_hod_feature, category_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products, prod_feature, prod_dow_feature, prod_hod_feature, category_feature = load_features(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_shard(shard=None, down_sample=None):    \n",
    "    priors, train, orders = load_data(root, down_sample=down_sample, aug=shard)\n",
    "    global products, prod_feature, prod_dow_feature, prod_hod_feature, category_feature\n",
    "\n",
    "    \n",
    "    orders['days_since_first_order'] = \\\n",
    "        orders.groupby('user_id').days_since_prior_order.cumsum().fillna(0)\n",
    "    orders = orders.merge(\n",
    "        orders.groupby('user_id').days_since_first_order.agg({'max_days':'max'}).reset_index(),\n",
    "        on = 'user_id', how = 'left')\n",
    "    orders['days_to_last_order'] = orders.max_days - orders.days_since_first_order\n",
    "    orders['hod_group'] = (orders.order_hour_of_day / 4).astype('int')\n",
    "    orders.drop(['days_since_first_order', 'max_days'], axis=1, inplace=True)\n",
    "    \n",
    "    priors_orders_detail = orders.merge(\n",
    "        right=priors, how='inner', on='order_id'\n",
    "    ).merge(\n",
    "        products[['product_id','aisle_id']], how = 'left', on = 'product_id'\n",
    "    )\n",
    "\n",
    "    # user features\n",
    "\n",
    "    agg_dict_2 = {'order_number':{'user_total_orders':'max'},\n",
    "                  'days_since_prior_order':{'user_sum_days_since_prior_order':'sum', \n",
    "                                            'user_mean_days_since_prior_order': 'mean'}}\n",
    "    users = ka_add_groupby_features_1_vs_n(orders[orders.eval_set == 'prior'], ['user_id'], agg_dict_2)\n",
    "\n",
    "    agg_dict_3 = {'reordered':\n",
    "                  {'user_reorder_ratio': \n",
    "                   lambda x: sum(priors_orders_detail.loc[x.index,'reordered']==1)/\n",
    "                             sum(priors_orders_detail.loc[x.index,'order_number'] > 1)},\n",
    "                  'product_id':{'user_total_products':'count', \n",
    "                                'user_distinct_products':'nunique'}}\n",
    "    us = ka_add_groupby_features_1_vs_n(priors_orders_detail, ['user_id'], agg_dict_3)\n",
    "    users = users.merge(us, how='inner', on = 'user_id')\n",
    "\n",
    "    users['user_average_basket'] = users.user_total_products / users.user_total_orders\n",
    "\n",
    "    us = orders[orders.eval_set != \"prior\"]\n",
    "    users = users.merge(us, how='inner', on = 'user_id')\n",
    "\n",
    "    # user-prod features\n",
    "    \n",
    "    agg_dict_4 = {'order_number':{'up_order_count': 'count', \n",
    "                                  'up_first_order_number': 'min', \n",
    "                                  'up_last_order_number':'max'}, \n",
    "                  'add_to_cart_order':{'up_average_cart_position': 'mean'},\n",
    "                  'days_to_last_order':{'up_days_since_last_order':'min'}}\n",
    "\n",
    "    data = ka_add_groupby_features_1_vs_n(\n",
    "        df=priors_orders_detail, \n",
    "        group_columns_list=['user_id', 'product_id', 'aisle_id'], \n",
    "        agg_dict=agg_dict_4)\n",
    "\n",
    "    # orders/days since last not order feature\n",
    "    \n",
    "    users.set_index('user_id', drop=False, inplace = True)\n",
    "    up_since_last_not_order = []\n",
    "    \n",
    "    for key, group in priors_orders_detail.groupby(['user_id', 'product_id']):\n",
    "        user_id = key[0]\n",
    "        current_total_order = users.loc[user_id].user_total_orders\n",
    "        \n",
    "        if set(range(1, current_total_order+1)) == set(group.order_number):\n",
    "            up_since_last_not_order.append(\n",
    "                {'user_id': user_id, \n",
    "                 'product_id': key[1], \n",
    "                 'up_order_since_last_not_order': None, \n",
    "                 'last_not_order_number': None})\n",
    "\n",
    "        else:\n",
    "            v = max(set(range(1, current_total_order+1)) - set(group.order_number))\n",
    "            up_since_last_not_order.append(\n",
    "                {'user_id': user_id, \n",
    "                 'product_id': key[1], \n",
    "                 'up_order_since_last_not_order': current_total_order - v + 1, \n",
    "                 'last_not_order_number': v})\n",
    "            \n",
    "    up_since_last_not_order_df = pd.DataFrame(up_since_last_not_order)\n",
    "    data = data.merge(up_since_last_not_order_df, how='left', on=['user_id', 'product_id'])\n",
    "    orders['last_not_order_number'] = orders.order_number\n",
    "    orders['up_days_since_last_not_order'] = orders.days_to_last_order\n",
    "    data = data.merge(\n",
    "        orders[['user_id', 'last_not_order_number', 'up_days_since_last_not_order']], \n",
    "        how='left', on=['user_id', 'last_not_order_number'])\n",
    "    \n",
    "    \n",
    "    # other pre-built features\n",
    "\n",
    "    data = data.merge(\n",
    "        prod_feature, how='inner', on='product_id'\n",
    "    ).merge(\n",
    "        users, how='inner', on='user_id'\n",
    "    ).merge(\n",
    "        category_feature, how = 'inner', on='aisle_id')\n",
    "\n",
    "    data['up_order_rate'] = data.up_order_count / data.user_total_orders\n",
    "    data['up_order_since_last_order'] = data.user_total_orders - data.up_last_order_number\n",
    "    data['up_order_rate_since_first_order'] = \\\n",
    "        data.up_order_count / (data.user_total_orders - data.up_first_order_number + 1)\n",
    "\n",
    "    # training labels\n",
    "\n",
    "    train = train.merge(right=orders[['order_id', 'user_id']], how='left', on='order_id')\n",
    "    data = data.merge(train[['user_id', 'product_id', 'reordered']], on=['user_id', 'product_id'], how='left')\n",
    "    \n",
    "    data = data.merge(\n",
    "        prod_hod_feature, \n",
    "        on = ['product_id', 'hod_group'], \n",
    "        how = 'left')\n",
    "    data.prod_market_share_hod.fillna(0)\n",
    "    data = data.merge(\n",
    "        prod_dow_feature, \n",
    "        on = ['product_id', 'order_dow'], how = 'left')\n",
    "    data.prod_market_share_dow.fillna(0)\n",
    "\n",
    "    # abt\n",
    "\n",
    "    drop_list = [\n",
    "        'user_id', 'aisle_id', 'order_number', 'order_dow', \n",
    "        'order_hour_of_day', 'days_to_last_order', 'hod_group', \n",
    "        'cat_num_of_prods_a_user_buys_in_this_cat_median', 'last_not_order_number']\n",
    "    data.drop(drop_list, inplace = True, axis=1)\n",
    "\n",
    "    for col in data.columns:\n",
    "        if data[col].dtypes == 'float64':\n",
    "            data[col] = data[col].astype('float32')\n",
    "        if data[col].dtypes == 'int64':\n",
    "            data[col] = data[col].astype('int32')\n",
    "\n",
    "    data_train = data[data.eval_set == 'train']\n",
    "    data_test = data[data.eval_set == 'test']\n",
    "    \n",
    "    print('Shard {} train'.format(shard), data_train.shape)\n",
    "    print('Shard {} test'.format(shard), data_test.shape)\n",
    "\n",
    "    if shard is None:\n",
    "        data_train.to_csv(os.path.join(root, 'abt', 'abt_train.csv'), index = None)\n",
    "        data_test.to_csv(os.path.join(root, 'abt', 'abt_test.csv'), index = None)\n",
    "    else:\n",
    "        data_train.to_csv(os.path.join(root, 'abt', 'abt_train.{}.csv'.format(shard)), index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_shards(shards, down_sample):\n",
    "    for s in shards:\n",
    "        with tick_tock(\"Process shard {}\".format(s)):\n",
    "            process_shard(shard=s, down_sample=down_sample)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process shard aug0-0 starts...\n",
      "Process shard aug0-0: 8.19s\n",
      "Process shard aug0-1 starts...\n",
      "Process shard aug0-1: 9.50s\n",
      "Process shard aug0-2 starts...\n",
      "Process shard aug0-2: 11.26s\n",
      "Process shard aug0-3 starts...\n",
      "Process shard aug0-3: 11.26s\n",
      "\n",
      "\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "down_sample = None\n",
    "n_shards = 32\n",
    "\n",
    "\n",
    "# n_shards = 1\n",
    "\n",
    "\n",
    "shards = [aug_name(s, ms) for ms in range(52) for s in range(4)]\n",
    "\n",
    "jobs = []\n",
    "for s in range(n_shards):\n",
    "    cur_shards = [shards[i] for i in range(len(shards)) if i%n_shards == s]\n",
    "    p = Process(target=process_shards, args=(cur_shards, down_sample))\n",
    "    p.start()\n",
    "    jobs.append(p)\n",
    "    \n",
    "for p in jobs:\n",
    "    p.join()\n",
    "\n",
    "print(\"\\n\\nAll done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_shards([None], down_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
