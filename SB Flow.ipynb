{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, os, gc, types\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_paths = [\n",
    "    \"/data/kaggle-instacart\",\n",
    "    \"/Users/jiayou/Dropbox/珺珺的程序/Kaggle/Instacart\",\n",
    "    \"/Users/jiayou/Dropbox/Documents/珺珺的程序/Kaggle/Instacart\"\n",
    "]\n",
    "root = None\n",
    "for p in root_paths:\n",
    "    if os.path.exists(p):\n",
    "        root = p\n",
    "        break\n",
    "path = root\n",
    "sbpath = os.path.join(root, 'sb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tick_tock:\n",
    "    def __init__(self, process_name, verbose=1):\n",
    "        self.process_name = process_name\n",
    "        self.verbose = verbose\n",
    "    def __enter__(self):\n",
    "        if self.verbose:\n",
    "            print(self.process_name + \" starts...\")\n",
    "            self.begin_time = time.time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.verbose:\n",
    "            end_time = time.time()\n",
    "            print('{} done: {:.2f}s'.format(self.process_name, end_time - self.begin_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import dok_matrix, coo_matrix\n",
    "from sklearn.utils.multiclass import  type_of_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gorders = pd.read_csv(os.path.join(path, \"orders.csv\"), dtype={'order_id': np.uint32,\n",
    "                                                              'user_id': np.uint32,\n",
    "                                                              'eval_set': 'category',\n",
    "                                                              'order_number': np.uint8,\n",
    "                                                              'order_dow': np.uint8,\n",
    "                                                              'order_hour_of_day': np.uint8\n",
    "                                                              })\n",
    "\n",
    "gorder_prior = pd.read_csv(os.path.join(path, \"order_products__prior.csv\"), dtype={'order_id': np.uint32,\n",
    "                                                                                  'product_id': np.uint16,\n",
    "                                                                                  'add_to_cart_order': np.uint8,\n",
    "                                                                                  'reordered': bool})\n",
    "gorder_prior = gorder_prior.merge(gorders[['order_id', 'user_id']], on='order_id', how='left')\n",
    "\n",
    "gorder_train = pd.read_csv(os.path.join(path, \"order_products__train.csv\"), dtype={'order_id': np.uint32,\n",
    "                                                                                  'product_id': np.uint16,\n",
    "                                                                                  'add_to_cart_order': np.uint8,\n",
    "                                                                                  'reordered': bool})\n",
    "gorder_train = gorder_train.merge(gorders[['order_id', 'user_id']], on='order_id', how='left')\n",
    "\n",
    "aisles = pd.read_csv(os.path.join(path, \"aisles.csv\"), dtype={'aisle_id': np.uint8, 'aisle':'category'})\n",
    "departments = pd.read_csv(\n",
    "    os.path.join(path, \"departments.csv\"), \n",
    "    dtype={'department_id':np.uint8, 'department': 'category'})\n",
    "products = pd.read_csv(os.path.join(path, \"products.csv\"), dtype={'product_id': np.uint16,\n",
    "                                                                  'aisle_id': np.uint8,\n",
    "                                                                  'department_id': np.uint8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug_name(s, ms):\n",
    "    return 'aug{}-{}'.format(s, ms)\n",
    "\n",
    "def orig_name(shard):\n",
    "    return 'orig{}'.format(shard)\n",
    "\n",
    "def load(shard=None, nshards=None, aug=None):\n",
    "    if aug is None:\n",
    "        global gorders, gorder_prior, gorder_train\n",
    "        orders = gorders.loc[gorders.user_id % nshards == shard, :]\n",
    "        \n",
    "        order_prior = gorder_prior.loc[gorder_prior.user_id % nshards == shard, :]\n",
    "        order_prior.drop('user_id', inplace = True, axis=1)\n",
    "        \n",
    "        order_train = gorder_train.loc[gorder_train.user_id % nshards == shard, :]\n",
    "        order_train.drop('user_id', inplace = True, axis=1)\n",
    "    else:\n",
    "        pf = os.path.join(root, 'aug', 'order_products__prior.{}.csv'.format(aug))\n",
    "        tf = os.path.join(root, 'aug', 'order_products__train.{}.csv'.format(aug))\n",
    "        of = os.path.join(root, 'aug', 'orders.{}.csv'.format(aug))\n",
    "        orders = pd.read_csv(of, dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'user_id': np.uint32,\n",
    "            'eval_set': 'category',\n",
    "            'order_number': np.uint8,\n",
    "            'order_dow': np.uint8,\n",
    "            'order_hour_of_day': np.uint8\n",
    "        })\n",
    "        order_prior = pd.read_csv(pf, dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'product_id': np.uint16,\n",
    "            'add_to_cart_order': np.uint8,\n",
    "            'reordered': bool\n",
    "        })\n",
    "        order_train = pd.read_csv(tf, dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'product_id': np.uint16,\n",
    "            'add_to_cart_order': np.uint8,\n",
    "            'reordered': bool\n",
    "        })\n",
    "    return (orders, order_prior, order_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_shard(s):\n",
    "    aug = s[0]\n",
    "    if aug:\n",
    "        orders, order_prior, order_train = load(aug=aug_name(s[1], s[2]))\n",
    "        sname = aug_name(s[1], s[2])\n",
    "    else:\n",
    "        orders, order_prior, order_train = load(shard=s[1], nshards=s[2])\n",
    "        sname = orig_name(s[1])\n",
    "        \n",
    "    global aisles, departments, products\n",
    "\n",
    "########## user's product list\n",
    "\n",
    "# def create_products():\n",
    "\n",
    "#     print('loaded')\n",
    "\n",
    "    torders = orders.loc[orders.eval_set == 'prior', :]\n",
    "    orders_user = torders[['order_id', 'user_id']]\n",
    "    previous_products = pd.merge(order_prior, orders_user, on='order_id')\n",
    "    previous_products = previous_products.loc[:, ['user_id', 'product_id']].drop_duplicates()\n",
    "\n",
    "#     print('save')\n",
    "#     print(labels.shape)\n",
    "#     print(labels.columns)\n",
    "#     labels.to_pickle('sb/previous_products.pkl')\n",
    "#     return labels\n",
    "\n",
    "# labels = create_products()\n",
    "\n",
    "\n",
    "\n",
    "########## abt skeleton without features\n",
    "\n",
    "# def split_data_set():\n",
    "    labels = previous_products.copy()\n",
    "    torders = orders.loc[(orders.eval_set == 'train') | (orders.eval_set == 'test'), :]\n",
    "    labels = pd.merge(labels, torders[['order_id', 'user_id', 'eval_set']], on='user_id').drop(['user_id'], axis=1)\n",
    "\n",
    "#     print('data is loaded')\n",
    "\n",
    "    current = np.unique(labels.order_id)\n",
    "    \n",
    "    current = labels.loc[np.in1d(labels.order_id, current), :]\n",
    "\n",
    "    current = pd.merge(\n",
    "        order_train.drop(['add_to_cart_order'], axis=1), \n",
    "        current, on=['order_id', 'product_id'], how='right')\n",
    "    current.reordered.fillna(False, inplace=True)\n",
    "    chunk = current\n",
    "\n",
    "# folds = split_data_set()\n",
    "\n",
    "\n",
    "\n",
    "########## intervals a user purchases a product\n",
    "\n",
    "# def orders_comsum():\n",
    "#     path = root\n",
    "#     sbpath = os.path.join(root, 'sb')\n",
    "\n",
    "    labels = chunk.copy()\n",
    "    user_product = previous_products.copy()\n",
    "\n",
    "    order_comsum = orders[['user_id', 'order_number', 'days_since_prior_order']].groupby(['user_id', 'order_number'])\\\n",
    "        ['days_since_prior_order'].sum().groupby(level=[0]).cumsum()\\\n",
    "        .reset_index().rename(columns={'days_since_prior_order':'days_since_prior_order_comsum'})\n",
    "\n",
    "    # order_comsum['days_since_prior_order_comsum'].fillna(0, inplace=True)\n",
    "#     order_comsum.to_pickle('sb/orders_comsum.pkl')\n",
    "\n",
    "    order_comsum = pd.merge(\n",
    "        order_comsum, orders, \n",
    "        on=['user_id', 'order_number'])[['user_id', 'order_number', 'days_since_prior_order_comsum', 'order_id']]\n",
    "\n",
    "    order_product = pd.merge(order_prior, orders, on='order_id')[['order_id', 'product_id', 'eval_set']]\n",
    "    order_product_train_test = labels[['order_id', 'product_id', 'eval_set']]\n",
    "\n",
    "    order_product = pd.concat([order_product, order_product_train_test])\n",
    "\n",
    "    order_product = pd.merge(order_product, order_comsum, on='order_id')\n",
    "\n",
    "#     print(order_product.columns)\n",
    "\n",
    "    order_product = pd.merge(order_product, user_product, on=['user_id', 'product_id'])\n",
    "\n",
    "    temp = order_product.groupby(['user_id', 'product_id', 'order_number'])\\\n",
    "        ['days_since_prior_order_comsum'].sum().groupby(level=[0, 1]).apply(lambda x: np.diff(np.nan_to_num(x)))\n",
    "    temp = temp.to_frame('periods').reset_index()\n",
    "\n",
    "#     temp.to_pickle('sb/product_period.pkl')\n",
    "\n",
    "    aggregated = temp.copy()\n",
    "    aggregated['last'] = aggregated.periods.apply(lambda x: x[-1])\n",
    "    aggregated['prev1'] = aggregated.periods.apply(lambda x: x[-2] if len(x) > 1 else np.nan)\n",
    "    aggregated['prev2'] = aggregated.periods.apply(lambda x: x[-3] if len(x) > 2 else np.nan)\n",
    "    aggregated['median'] = aggregated.periods.apply(lambda x: np.median(x[:-1]))\n",
    "    aggregated['mean'] = aggregated.periods.apply(lambda x: np.mean(x[:-1]))\n",
    "    aggregated.drop('periods', axis=1, inplace=True)\n",
    "\n",
    "    if aug:\n",
    "        aggregated.to_pickle('abt/feature.up_interval_stat.{}.pkl'.format(sname))\n",
    "    else:\n",
    "        aggregated.to_pickle('abt-share/feature.up_interval_stat.{}.pkl'.format(sname))\n",
    "    \n",
    "#     return (order_comsum, temp, aggregated)\n",
    "\n",
    "# oc = orders_comsum()\n",
    "\n",
    "\n",
    "\n",
    "########## user_dep_stat and user_aisle_stat\n",
    "\n",
    "# def user_product_rank():\n",
    "#     path = root\n",
    "#     sbpath = os.path.join(root, 'sb')\n",
    "\n",
    "    order_train = chunk.copy()\n",
    "    \n",
    "    orders_products = pd.merge(orders, order_prior, on=\"order_id\")\n",
    "\n",
    "    orders_products_products = pd.merge(orders_products, products[['product_id', 'department_id', 'aisle_id']],\n",
    "                                        on='product_id')\n",
    "\n",
    "    user_dep_stat = orders_products_products.groupby(['user_id', 'department_id']).agg(\n",
    "        {'product_id': lambda x: x.nunique(),\n",
    "         'reordered': 'sum'\n",
    "         })\n",
    "    user_dep_stat.rename(columns={'product_id': 'dep_products',\n",
    "                                  'reordered': 'dep_reordered'}, inplace=True)\n",
    "    user_dep_stat.reset_index(inplace=True)\n",
    "#     print(user_dep_stat.columns)\n",
    "#     user_dep_stat.to_pickle('abt-share/feature.user_department.{}.pkl'.format(sname))\n",
    "\n",
    "    user_aisle_stat = orders_products_products.groupby(['user_id', 'aisle_id']).agg(\n",
    "        {'product_id': lambda x: x.nunique(),\n",
    "         'reordered': 'sum'\n",
    "         })\n",
    "    user_aisle_stat.rename(columns={'product_id': 'aisle_products',\n",
    "                                    'reordered': 'aisle_reordered'}, inplace=True)\n",
    "    user_aisle_stat.reset_index(inplace=True)\n",
    "#     print(user_aisle_stat.columns)\n",
    "#     user_aisle_stat.to_pickle('abt-share/feature.user_aisle.{}.pkl'.format(sname))\n",
    "    \n",
    "#     return (user_dep_stat, user_aisle_stat)\n",
    "\n",
    "# upr = user_product_rank()\n",
    "\n",
    "\n",
    "\n",
    "########## other features\n",
    "\n",
    "    order_train = chunk.copy()\n",
    "    order_test = order_train.loc[order_train.eval_set == \"test\", ['order_id', 'product_id']]\n",
    "    order_train = order_train.loc[order_train.eval_set == \"train\", ['order_id',  'product_id',  'reordered']]\n",
    "\n",
    "    ###########################\n",
    "\n",
    "#     prob = pd.merge(order_prior, orders, on='order_id')\n",
    "# #     print(prob.columns)\n",
    "#     prob = prob.groupby(['product_id', 'user_id'])\\\n",
    "#         .agg({'reordered':'sum', 'user_id': 'size'})\n",
    "# #     print(prob.columns)\n",
    "\n",
    "#     prob.rename(columns={'sum': 'reordered',\n",
    "#                          'user_id': 'total'}, inplace=True)\n",
    "\n",
    "#     prob.reordered = (prob.reordered > 0).astype(np.float32)\n",
    "#     prob.total = (prob.total > 0).astype(np.float32)\n",
    "#     prob['reorder_prob'] = prob.reordered / prob.total\n",
    "#     prob = prob.groupby('product_id').agg({'reorder_prob': 'mean'}).rename(columns={'mean': 'reorder_prob'})\\\n",
    "#         .reset_index()\n",
    "\n",
    "\n",
    "#     prod_stat = order_prior.groupby('product_id').agg({'reordered': ['sum', 'size'],\n",
    "#                                                        'add_to_cart_order':'mean'})\n",
    "#     prod_stat.columns = prod_stat.columns.levels[1]\n",
    "#     prod_stat.rename(columns={'sum':'prod_reorders',\n",
    "#                               'size':'prod_orders',\n",
    "#                               'mean': 'prod_add_to_card_mean'}, inplace=True)\n",
    "#     prod_stat.reset_index(inplace=True)\n",
    "\n",
    "#     prod_stat['reorder_ration'] = prod_stat['prod_reorders'] / prod_stat['prod_orders']\n",
    "\n",
    "#     prod_stat = pd.merge(prod_stat, prob, on='product_id')\n",
    "\n",
    "#     # prod_stat.drop(['prod_reorders'], axis=1, inplace=True)\n",
    "\n",
    "    user_stat = orders.loc[orders.eval_set == 'prior', :].groupby('user_id').agg({'order_number': 'max',\n",
    "                                                                                  'days_since_prior_order': ['sum',\n",
    "                                                                                                             'mean',\n",
    "                                                                                                             'median']})\n",
    "    user_stat.columns = user_stat.columns.droplevel(0)\n",
    "    user_stat.rename(columns={'max': 'user_orders',\n",
    "                              'sum': 'user_order_starts_at',\n",
    "                              'mean': 'user_mean_days_since_prior',\n",
    "                              'median': 'user_median_days_since_prior'}, inplace=True)\n",
    "    user_stat.reset_index(inplace=True)\n",
    "\n",
    "    orders_products = pd.merge(orders, order_prior, on=\"order_id\")\n",
    "\n",
    "    user_order_stat = orders_products.groupby('user_id').agg({'user_id': 'size',\n",
    "                                                              'reordered': 'sum',\n",
    "                                                              \"product_id\": lambda x: x.nunique()})\n",
    "\n",
    "    user_order_stat.rename(columns={'user_id': 'user_total_products',\n",
    "                                    'product_id': 'user_distinct_products',\n",
    "                                    'reordered': 'user_reorder_ratio'}, inplace=True)\n",
    "\n",
    "    user_order_stat.reset_index(inplace=True)\n",
    "    user_order_stat.user_reorder_ratio = user_order_stat.user_reorder_ratio / user_order_stat.user_total_products\n",
    "\n",
    "    user_stat = pd.merge(user_stat, user_order_stat, on='user_id')\n",
    "    user_stat['user_average_basket'] = user_stat.user_total_products / user_stat.user_orders\n",
    "\n",
    "    ########################### products\n",
    "\n",
    "#     prod_usr = orders_products.groupby(['product_id']).agg({'user_id': lambda x: x.nunique()})\n",
    "#     prod_usr.rename(columns={'user_id':'prod_users_unq'}, inplace=True)\n",
    "#     prod_usr.reset_index(inplace=True)\n",
    "\n",
    "#     prod_usr_reordered = orders_products.loc[orders_products.reordered, :]\\\n",
    "#         .groupby(['product_id']).agg({'user_id': lambda x: x.nunique()})\n",
    "#     prod_usr_reordered.rename(columns={'user_id': 'prod_users_unq_reordered'}, inplace=True)\n",
    "#     prod_usr_reordered.reset_index(inplace=True)\n",
    "\n",
    "    order_stat = orders_products.groupby('order_id').agg({'order_id': 'size'}) \\\n",
    "        .rename(columns={'order_id': 'order_size'}).reset_index()\n",
    "\n",
    "    orders_products = pd.merge(orders_products, order_stat, on='order_id')\n",
    "    orders_products['add_to_cart_order_inverted'] = orders_products.order_size - orders_products.add_to_cart_order\n",
    "    orders_products['add_to_cart_order_relative'] = orders_products.add_to_cart_order / orders_products.order_size\n",
    "\n",
    "    data = orders_products.groupby(['user_id', 'product_id']).agg({'user_id': 'size',\n",
    "                                                                   'order_number': ['min', 'max'],\n",
    "                                                                   'add_to_cart_order': ['mean', 'median'],\n",
    "                                                                   'days_since_prior_order': ['mean', 'median'],\n",
    "                                                                   'order_dow': ['mean', 'median'],\n",
    "                                                                   'order_hour_of_day': ['mean', 'median'],\n",
    "                                                                   'add_to_cart_order_inverted': ['mean', 'median'],\n",
    "                                                                   'add_to_cart_order_relative': ['mean', 'median'],\n",
    "                                                                   'reordered': ['sum']})\n",
    "\n",
    "    data.columns = data.columns.droplevel(0)\n",
    "    data.columns = ['up_orders', 'up_first_order', 'up_last_order', 'up_mean_cart_position', 'up_median_cart_position',\n",
    "                    'days_since_prior_order_mean', 'days_since_prior_order_median', 'order_dow_mean',\n",
    "                    'order_dow_median',\n",
    "                    'order_hour_of_day_mean', 'order_hour_of_day_median',\n",
    "                    'add_to_cart_order_inverted_mean', 'add_to_cart_order_inverted_median',\n",
    "                    'add_to_cart_order_relative_mean', 'add_to_cart_order_relative_median',\n",
    "                    'reordered_sum'\n",
    "                    ]\n",
    "\n",
    "    data['user_product_reordered_ratio'] = (data.reordered_sum + 1.0) / data.up_orders\n",
    "\n",
    "    # data['first_order'] = data['up_orders'] > 0\n",
    "    # data['second_order'] = data['up_orders'] > 1\n",
    "    #\n",
    "    # data.groupby('product_id')['']\n",
    "\n",
    "    data.reset_index(inplace=True)\n",
    "\n",
    "#     data = pd.merge(data, prod_stat, on='product_id')\n",
    "    data = pd.merge(data, user_stat, on='user_id')\n",
    "\n",
    "    data['up_order_rate'] = data.up_orders / data.user_orders\n",
    "    data['up_orders_since_last_order'] = data.user_orders - data.up_last_order\n",
    "    data['up_order_rate_since_first_order'] = data.user_orders / (data.user_orders - data.up_first_order + 1)\n",
    "\n",
    "    ############################\n",
    "\n",
    "#     user_dep_stat = pd.read_pickle('data/user_department_products.pkl')\n",
    "#     user_aisle_stat = pd.read_pickle('data/user_aisle_products.pkl')\n",
    "\n",
    "    ############### train\n",
    "\n",
    "#     print(order_train.shape)\n",
    "    order_train = pd.merge(order_train, products, on='product_id')\n",
    "#     print(order_train.shape)\n",
    "    order_train = pd.merge(order_train, orders, on='order_id')\n",
    "#     print(order_train.shape)\n",
    "    order_train = pd.merge(order_train, user_dep_stat, on=['user_id', 'department_id'])\n",
    "#     print(order_train.shape)\n",
    "    order_train = pd.merge(order_train, user_aisle_stat, on=['user_id', 'aisle_id'])\n",
    "#     print(order_train.shape)\n",
    "\n",
    "#     order_train = pd.merge(order_train, prod_usr, on='product_id')\n",
    "#     print(order_train.shape)\n",
    "#     order_train = pd.merge(order_train, prod_usr_reordered, on='product_id', how='left')\n",
    "#     order_train.prod_users_unq_reordered.fillna(0, inplace=True)\n",
    "#     print(order_train.shape)\n",
    "\n",
    "    order_train = pd.merge(order_train, data, on=['product_id', 'user_id'])\n",
    "#     print(order_train.shape)\n",
    "\n",
    "    order_train['aisle_reordered_ratio'] = order_train.aisle_reordered / order_train.user_orders\n",
    "    order_train['dep_reordered_ratio'] = order_train.dep_reordered / order_train.user_orders\n",
    "\n",
    "#     order_train = pd.merge(order_train, product_periods, on=['user_id',  'product_id'])\n",
    "\n",
    "    if aug:\n",
    "        order_train.to_pickle(os.path.join(root, 'abt', 'abt_train.sb.{}.pkl'.format(sname)))\n",
    "    else:\n",
    "        order_train.to_pickle(os.path.join(root, 'abt-share', 'abt_train.sb.{}.pkl'.format(sname)))\n",
    "\n",
    "    ##############\n",
    "\n",
    "    if not aug:\n",
    "        order_test = pd.merge(order_test, products, on='product_id')\n",
    "        order_test = pd.merge(order_test, orders, on='order_id')\n",
    "        order_test = pd.merge(order_test, user_dep_stat, on=['user_id', 'department_id'])\n",
    "        order_test = pd.merge(order_test, user_aisle_stat, on=['user_id', 'aisle_id'])\n",
    "\n",
    "#         order_test = pd.merge(order_test, prod_usr, on='product_id')\n",
    "#         order_test = pd.merge(order_test, prod_usr_reordered, on='product_id', how='left')\n",
    "#         order_train.prod_users_unq_reordered.fillna(0, inplace=True)\n",
    "\n",
    "        order_test = pd.merge(order_test, data, on=['product_id', 'user_id'])\n",
    "\n",
    "        order_test['aisle_reordered_ratio'] = order_test.aisle_reordered / order_test.user_orders\n",
    "        order_test['dep_reordered_ratio'] = order_test.dep_reordered / order_test.user_orders\n",
    "\n",
    "#         order_test = pd.merge(order_test, product_periods, on=['user_id', 'product_id'])\n",
    "    \n",
    "        order_test.to_pickle(os.path.join(root, 'abt-share', 'abt_test.sb.{}.pkl'.format(sname)))\n",
    "\n",
    "#     order_train = pd.merge(order_train, product_embeddings, on=['product_id'])\n",
    "#     order_test = pd.merge(order_test, product_embeddings, on=['product_id'])\n",
    "\n",
    "\n",
    "    return (aggregated, order_train, order_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_shards(shards):\n",
    "    for s in shards:\n",
    "        with tick_tock(\"Process shard {}\".format(s)):\n",
    "            process_shard(shard=s)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "n_processes = 32\n",
    "\n",
    "shards = [(False, s, 64) for s in range(64)] + [(True, s, ms) for ms in range(2) for s in range(32)]\n",
    "\n",
    "jobs = []\n",
    "for pid in range(n_processes):\n",
    "    cur_shards = [shards[i] for i in range(len(shards)) if i % n_processes == pid]\n",
    "    p = Process(target=process_shards, args=(cur_shards, down_sample))\n",
    "    p.start()\n",
    "    jobs.append(p)\n",
    "    \n",
    "for p in jobs:\n",
    "    p.join()\n",
    "\n",
    "print(\"\\n\\nAll done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiayou/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "r = process_shard((True, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>100000000.0</td>\n",
       "      <td>100000000.0</td>\n",
       "      <td>100000000.0</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>100000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <td>23.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>2.002000e+03</td>\n",
       "      <td>2573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>115.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>7.100000e+01</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.5</td>\n",
       "      <td>1.466667e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0            1            2             3            4\n",
       "user_id     100000000.0  100000000.0  100000000.0  1.000000e+08  100000000.0\n",
       "product_id         23.0         79.0       1559.0  2.002000e+03       2573.0\n",
       "last              115.0         13.0         43.0  7.100000e+01        101.0\n",
       "prev1               NaN          NaN         28.0  3.000000e+01          NaN\n",
       "prev2               NaN          NaN         30.0  6.000000e+00          NaN\n",
       "median              NaN          NaN         29.0  8.000000e+00          NaN\n",
       "mean                NaN          NaN         31.5  1.466667e+01          NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
