{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, os, gc, types\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_paths = [\n",
    "    \"/data/kaggle-instacart\",\n",
    "    \"/Users/jiayou/Dropbox/珺珺的程序/Kaggle/Instacart\",\n",
    "    \"/Users/jiayou/Dropbox/Documents/珺珺的程序/Kaggle/Instacart\"\n",
    "]\n",
    "root = None\n",
    "for p in root_paths:\n",
    "    if os.path.exists(p):\n",
    "        root = p\n",
    "        break\n",
    "path = root\n",
    "sbpath = os.path.join(root, 'sb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tick_tock:\n",
    "    def __init__(self, process_name, verbose=1):\n",
    "        self.process_name = process_name\n",
    "        self.verbose = verbose\n",
    "    def __enter__(self):\n",
    "        if self.verbose:\n",
    "            print(self.process_name + \" starts...\")\n",
    "            self.begin_time = time.time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.verbose:\n",
    "            end_time = time.time()\n",
    "            print('{} done: {:.2f}s'.format(self.process_name, end_time - self.begin_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import dok_matrix, coo_matrix\n",
    "from sklearn.utils.multiclass import  type_of_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gorders = pd.read_csv(os.path.join(path, \"orders.csv\"), dtype={'order_id': np.uint32,\n",
    "                                                              'user_id': np.uint32,\n",
    "                                                              'eval_set': 'category',\n",
    "                                                              'order_number': np.uint8,\n",
    "                                                              'order_dow': np.uint8,\n",
    "                                                              'order_hour_of_day': np.uint8\n",
    "                                                              })\n",
    "\n",
    "gorder_prior = pd.read_csv(os.path.join(path, \"order_products__prior.csv\"), dtype={'order_id': np.uint32,\n",
    "                                                                                  'product_id': np.uint16,\n",
    "                                                                                  'add_to_cart_order': np.uint8,\n",
    "                                                                                  'reordered': bool})\n",
    "gorder_prior = gorder_prior.merge(gorders[['order_id', 'user_id']], on='order_id', how='left')\n",
    "\n",
    "gorder_train = pd.read_csv(os.path.join(path, \"order_products__train.csv\"), dtype={'order_id': np.uint32,\n",
    "                                                                                  'product_id': np.uint16,\n",
    "                                                                                  'add_to_cart_order': np.uint8,\n",
    "                                                                                  'reordered': bool})\n",
    "gorder_train = gorder_train.merge(gorders[['order_id', 'user_id']], on='order_id', how='left')\n",
    "\n",
    "aisles = pd.read_csv(os.path.join(path, \"aisles.csv\"), dtype={'aisle_id': np.uint8, 'aisle':'category'})\n",
    "departments = pd.read_csv(\n",
    "    os.path.join(path, \"departments.csv\"), \n",
    "    dtype={'department_id':np.uint8, 'department': 'category'})\n",
    "products = pd.read_csv(os.path.join(path, \"products.csv\"), dtype={'product_id': np.uint16,\n",
    "                                                                  'aisle_id': np.uint8,\n",
    "                                                                  'department_id': np.uint8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug_name(s, ms):\n",
    "    return 'aug{}-{}'.format(s, ms)\n",
    "\n",
    "def orig_name(shard):\n",
    "    return 'orig{}'.format(shard)\n",
    "\n",
    "def load(shard=None, nshards=None, aug=None):\n",
    "    if aug is None:\n",
    "        global gorders, gorder_prior, gorder_train\n",
    "        orders = gorders.loc[gorders.user_id % nshards == shard, :]\n",
    "        \n",
    "        order_prior = gorder_prior.loc[gorder_prior.user_id % nshards == shard, :]\n",
    "        order_prior.drop('user_id', inplace = True, axis=1)\n",
    "        \n",
    "        order_train = gorder_train.loc[gorder_train.user_id % nshards == shard, :]\n",
    "        order_train.drop('user_id', inplace = True, axis=1)\n",
    "    else:\n",
    "        pf = os.path.join(root, 'aug', 'order_products__prior.{}.csv'.format(aug))\n",
    "        tf = os.path.join(root, 'aug', 'order_products__train.{}.csv'.format(aug))\n",
    "        of = os.path.join(root, 'aug', 'orders.{}.csv'.format(aug))\n",
    "        orders = pd.read_csv(of, dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'user_id': np.uint32,\n",
    "            'eval_set': 'category',\n",
    "            'order_number': np.uint8,\n",
    "            'order_dow': np.uint8,\n",
    "            'order_hour_of_day': np.uint8\n",
    "        })\n",
    "        order_prior = pd.read_csv(pf, dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'product_id': np.uint16,\n",
    "            'add_to_cart_order': np.uint8,\n",
    "            'reordered': bool\n",
    "        })\n",
    "        order_train = pd.read_csv(tf, dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'product_id': np.uint16,\n",
    "            'add_to_cart_order': np.uint8,\n",
    "            'reordered': bool\n",
    "        })\n",
    "    return (orders, order_prior, order_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_shard(s):\n",
    "    aug = s[0]\n",
    "    if aug:\n",
    "        orders, order_prior, order_train = load(aug=aug_name(s[1], s[2]))\n",
    "        name = aug_name(s[1], s[2])\n",
    "    else:\n",
    "        orders, order_prior, order_train = load(shard=s[1], nshards=s[2])\n",
    "        name = orig_name(s[1])\n",
    "        \n",
    "    global aisles, departments, products\n",
    "\n",
    "########## user's product list\n",
    "\n",
    "# def create_products():\n",
    "\n",
    "#     print('loaded')\n",
    "\n",
    "    torders = orders.loc[orders.eval_set == 'prior', :]\n",
    "    orders_user = torders[['order_id', 'user_id']]\n",
    "    previous_products = pd.merge(order_prior, orders_user, on='order_id')\n",
    "    previous_products = previous_products.loc[:, ['user_id', 'product_id']].drop_duplicates()\n",
    "\n",
    "#     print('save')\n",
    "#     print(labels.shape)\n",
    "#     print(labels.columns)\n",
    "#     labels.to_pickle('sb/previous_products.pkl')\n",
    "#     return labels\n",
    "\n",
    "# labels = create_products()\n",
    "\n",
    "\n",
    "\n",
    "########## abt skeleton without features\n",
    "\n",
    "# def split_data_set():\n",
    "    labels = previous_products.copy()\n",
    "    torders = orders.loc[(orders.eval_set == 'train') | (orders.eval_set == 'test'), :]\n",
    "    labels = pd.merge(labels, torders[['order_id', 'user_id', 'eval_set']], on='user_id').drop(['user_id'], axis=1)\n",
    "\n",
    "#     print('data is loaded')\n",
    "\n",
    "    torders = np.unique(labels.order_id)\n",
    "\n",
    "    size = torders.shape[0] // folds\n",
    "\n",
    "\n",
    "    current = torders\n",
    "\n",
    "    current = labels.loc[np.in1d(labels.order_id, current), :]\n",
    "\n",
    "    current = pd.merge(\n",
    "        order_train.drop(['add_to_cart_order'], axis=1), \n",
    "        current, on=['order_id', 'product_id'], how='right')\n",
    "    current.reordered.fillna(False, inplace=True)\n",
    "    chunk = current\n",
    "\n",
    "# folds = split_data_set()\n",
    "\n",
    "\n",
    "\n",
    "########## intervals a user purchases a product\n",
    "\n",
    "# def orders_comsum():\n",
    "#     path = root\n",
    "#     sbpath = os.path.join(root, 'sb')\n",
    "\n",
    "    aisles = pd.read_csv(os.path.join(path, \"aisles.csv\"), dtype={'aisle_id': np.uint8, 'aisle': 'category'})\n",
    "    departments = pd.read_csv(os.path.join(path, \"departments.csv\"),\n",
    "                              dtype={'department_id': np.uint8, 'department': 'category'})\n",
    "    orders, order_prior, order_train = load()\n",
    "\n",
    "    products = pd.read_csv(os.path.join(path, \"products.csv\"), dtype={'product_id': np.uint16,\n",
    "                                                                      'aisle_id': np.uint8,\n",
    "                                                                      'department_id': np.uint8})\n",
    "\n",
    "    labels = pd.read_pickle(os.path.join(sbpath, 'chunk_0.pkl'))\n",
    "    user_product = previous_products.copy()\n",
    "\n",
    "    order_comsum = orders[['user_id', 'order_number', 'days_since_prior_order']].groupby(['user_id', 'order_number'])\\\n",
    "            ['days_since_prior_order'].sum().groupby(level=[0]).cumsum().reset_index().rename(columns={'days_since_prior_order':'days_since_prior_order_comsum'})\n",
    "\n",
    "    # order_comsum['days_since_prior_order_comsum'].fillna(0, inplace=True)\n",
    "    order_comsum.to_pickle('sb/orders_comsum.pkl')\n",
    "\n",
    "    order_comsum2 = pd.merge(order_comsum, orders, on=['user_id', 'order_number'])[['user_id', 'order_number', 'days_since_prior_order_comsum', 'order_id']]\n",
    "\n",
    "    order_product = pd.merge(order_prior, orders, on='order_id')[['order_id', 'product_id', 'eval_set']]\n",
    "    order_product_train_test = labels[['order_id', 'product_id', 'eval_set']]\n",
    "\n",
    "    order_product = pd.concat([order_product, order_product_train_test])\n",
    "\n",
    "    order_product = pd.merge(order_product, order_comsum2, on='order_id')\n",
    "\n",
    "    print(order_product.columns)\n",
    "\n",
    "    order_product = pd.merge(order_product, user_product, on=['user_id', 'product_id'])\n",
    "\n",
    "    temp = order_product.groupby(['user_id', 'product_id', 'order_number'])['days_since_prior_order_comsum'].sum().groupby(level=[0, 1]).apply(lambda x: np.diff(np.nan_to_num(x)))\n",
    "    temp = temp.to_frame('periods').reset_index()\n",
    "\n",
    "    temp.to_pickle('sb/product_period.pkl')\n",
    "\n",
    "    aggregated = temp.copy()\n",
    "    aggregated['last'] = aggregated.periods.apply(lambda x: x[-1])\n",
    "    aggregated['prev1'] = aggregated.periods.apply(lambda x: x[-2] if len(x) > 1 else np.nan)\n",
    "    aggregated['prev2'] = aggregated.periods.apply(lambda x: x[-3] if len(x) > 2 else np.nan)\n",
    "    aggregated['median'] = aggregated.periods.apply(lambda x: np.median(x[:-1]))\n",
    "    aggregated['mean'] = aggregated.periods.apply(lambda x: np.mean(x[:-1]))\n",
    "    aggregated.drop('periods', axis=1, inplace=True)\n",
    "\n",
    "    aggregated.to_pickle('sb/product_periods_stat.pkl')\n",
    "    \n",
    "    return (order_comsum, temp, aggregated)\n",
    "\n",
    "oc = orders_comsum()\n",
    "\n",
    "\n",
    "\n",
    "########## user_dep_stat and user_aisle_stat\n",
    "\n",
    "\n",
    "\n",
    "def user_product_rank():\n",
    "    path = root\n",
    "    sbpath = os.path.join(root, 'sb')\n",
    "\n",
    "    aisles = pd.read_csv(os.path.join(path, \"aisles.csv\"), dtype={'aisle_id': np.uint8, 'aisle': 'category'})\n",
    "    departments = pd.read_csv(os.path.join(path, \"departments.csv\"),\n",
    "                              dtype={'department_id': np.uint8, 'department': 'category'})\n",
    "\n",
    "    orders, order_prior, order_train = load()\n",
    "\n",
    "    products = pd.read_csv(os.path.join(path, \"products.csv\"), dtype={'product_id': np.uint16,\n",
    "                                                                      'aisle_id': np.uint8,\n",
    "                                                                      'department_id': np.uint8})\n",
    "\n",
    "    order_train = pd.read_pickle(os.path.join(sbpath, 'chunk_0.pkl'))\n",
    "\n",
    "    \n",
    "    orders_products = pd.merge(orders, order_prior, on=\"order_id\")\n",
    "\n",
    "    orders_products_products = pd.merge(orders_products, products[['product_id', 'department_id', 'aisle_id']],\n",
    "                                        on='product_id')\n",
    "\n",
    "    user_dep_stat = orders_products_products.groupby(['user_id', 'department_id']).agg(\n",
    "        {'product_id': lambda x: x.nunique(),\n",
    "         'reordered': 'sum'\n",
    "         })\n",
    "    user_dep_stat.rename(columns={'product_id': 'dep_products',\n",
    "                                  'reordered': 'dep_reordered'}, inplace=True)\n",
    "    user_dep_stat.reset_index(inplace=True)\n",
    "    print(user_dep_stat.columns)\n",
    "    user_dep_stat.to_pickle('sb/user_department_products.pkl')\n",
    "\n",
    "    user_aisle_stat = orders_products_products.groupby(['user_id', 'aisle_id']).agg(\n",
    "        {'product_id': lambda x: x.nunique(),\n",
    "         'reordered': 'sum'\n",
    "         })\n",
    "    user_aisle_stat.rename(columns={'product_id': 'aisle_products',\n",
    "                                    'reordered': 'aisle_reordered'}, inplace=True)\n",
    "    user_aisle_stat.reset_index(inplace=True)\n",
    "    print(user_aisle_stat.columns)\n",
    "    user_aisle_stat.to_pickle('sb/user_aisle_products.pkl')\n",
    "    \n",
    "    return (user_dep_stat, user_aisle_stat)\n",
    "\n",
    "upr = user_product_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_shards(shards):\n",
    "    for s in shards:\n",
    "        with tick_tock(\"Process shard {}\".format(s)):\n",
    "            process_shard(shard=s)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "n_shards = 32\n",
    "\n",
    "shards = [(False, s, 64) for s in range(64)] + [(True, s, ms) for ms in range(2) for s in range(32)]\n",
    "\n",
    "jobs = []\n",
    "for s in range(n_shards):\n",
    "    cur_shards = [shards[i] for i in range(len(shards)) if i%n_shards == s]\n",
    "    p = Process(target=process_shards, args=(cur_shards, down_sample))\n",
    "    p.start()\n",
    "    jobs.append(p)\n",
    "    \n",
    "for p in jobs:\n",
    "    p.join()\n",
    "\n",
    "print(\"\\n\\nAll done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
